{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568db971-8507-4a82-a0d9-b80df46ec669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9536e502-b217-475d-93c8-97470c18c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_1_path = r\"C:\\Users\\Charitha\\Downloads\\training set 1.zip\"\n",
    "train_set_2_path = r\"C:\\Users\\Charitha\\Downloads\\training set 2.zip\"\n",
    "test_set_path = r\"C:\\Users\\Charitha\\Downloads\\test.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842cc5cc-1692-4949-9bcf-d9c6f4f2becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load images and annotations from Training Set 1\n",
    "def load_training_set_1(path):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith('.jpg'):\n",
    "            img_path = os.path.join(path, filename)\n",
    "            images.append(cv2.imread(img_path))\n",
    "            annotation_path = os.path.join(path, filename.replace('.jpg', '.json'))\n",
    "            with open(annotation_path, 'r') as f:\n",
    "                annotations.append(json.load(f))\n",
    "    return images, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8585e4-5864-4f12-8fbf-6b7efc003d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize a sample image with bounding box from Training Set 1\n",
    "def visualize_training_set_1(image, annotation):\n",
    "    ymin, xmin, ymax, xmax = annotation['bbox']\n",
    "    image = cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (255, 0, 0), 2)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Training Set 1 - Bounding Box\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8292d6a2-57c2-4458-87d5-166a64018e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize a sample image with text annotation from Training Set 2\n",
    "def visualize_training_set_2(image, annotation):\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Training Set 2 - License Plate: {annotation}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a84c8f-59af-48c2-9b00-664940480cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze annotations in Training Set 1\n",
    "def analyze_annotations_1(annotations):\n",
    "    bbox_data = []\n",
    "    for ann in annotations:\n",
    "        bbox_data.append(ann['bbox'])\n",
    "    df = pd.DataFrame(bbox_data, columns=['ymin', 'xmin', 'ymax', 'xmax'])\n",
    "    sns.pairplot(df)\n",
    "    plt.title(\"Bounding Box Coordinates Distribution\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0798da2-f14d-4820-a874-2b39c891732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze annotations in Training Set 2\n",
    "def analyze_annotations_2(annotations):\n",
    "    lengths = [len(ann) for ann in annotations]\n",
    "    plt.hist(lengths, bins=20)\n",
    "    plt.title(\"Length of License Plate Text Annotations\")\n",
    "    plt.xlabel(\"Number of Characters\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4888186-8a9c-4a42-b0c5-e817e472f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas opencv-python matplotlib torch torchvision tensorflow scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d629b-c7a9-4092-a3e5-394e87798c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations\n",
    "annotations = pd.read_csv('annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a663d0e-ad66-46e8-aea0-705e1526a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "images = {}\n",
    "for filename in os.listdir('images'):\n",
    "    img = cv2.imread(os.path.join('images', filename))\n",
    "    if img is not None:\n",
    "        images[filename] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ddf64f-65ba-46b6-8a1c-38429dc78943",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43bf5ce-9cdc-467e-b2de-864476dfb60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample image with its bounding box\n",
    "sample_image = images['sample.jpg']\n",
    "annotation = annotations[annotations['filename'] == 'sample.jpg']\n",
    "\n",
    "ymin, xmin, ymax, xmax = annotation[['ymin', 'xmin', 'ymax', 'xmax']].values[0]\n",
    "\n",
    "plt.imshow(cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB))\n",
    "plt.gca().add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=2, edgecolor='r', facecolor='none'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133ca4f3-060a-4261-bc8b-8981204b655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse bounding box coordinates\n",
    "annotations['bbox'] = annotations.apply(lambda row: (row['xmin'], row['ymin'], row['xmax'], row['ymax']), axis=1)\n",
    "annotations.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d7ceee-7b07-47c3-bd2b-20087414514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics for bounding boxes\n",
    "bbox_stats = annotations[['xmin', 'ymin', 'xmax', 'ymax']].describe()\n",
    "print(bbox_stats)\n",
    "\n",
    "# Frequency of characters in license plates\n",
    "char_freq = pd.Series(''.join(annotations['text'].values)).value_counts()\n",
    "print(char_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccecd55-9d33-480a-9bfe-0edf96c7b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bounding box size distribution\n",
    "bbox_sizes = annotations.apply(lambda row: (row['xmax'] - row['xmin']) * (row['ymax'] - row['ymin']), axis=1)\n",
    "plt.hist(bbox_sizes, bins=50)\n",
    "plt.xlabel('Bounding Box Size')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Plot character frequency distribution\n",
    "plt.bar(char_freq.index, char_freq.values)\n",
    "plt.xlabel('Character')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcbea86-31fb-44e8-8a44-743bbabaa1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model and modify it for our use case\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = 2  # Background + license plate\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Define training loop (simplified)\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "class LicensePlateDataset(Dataset):\n",
    "    def __init__(self, annotations, images):\n",
    "        self.annotations = annotations\n",
    "        self.images = images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.annotations.iloc[idx]['filename']\n",
    "        image = self.images[img_name]\n",
    "        bbox = self.annotations.iloc[idx]['bbox']\n",
    "        target = {\n",
    "            'boxes': torch.tensor([bbox], dtype=torch.float32),\n",
    "            'labels': torch.tensor([1], dtype=torch.int64)\n",
    "        }\n",
    "        return image, target\n",
    "\n",
    "train_dataset = LicensePlateDataset(annotations, images)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch} Loss: {losses.item()}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352dfcc-17d8-4809-b614-807af12eaed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Reshape, Dense, LSTM\n",
    "\n",
    "# Example of a simple CRNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 128, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Reshape((32, 64*64)),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    LSTM(128),\n",
    "    Dense(36, activation='softmax')  # 36 classes for alphanumeric characters\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Dummy data for example purposes\n",
    "train_images = np.random.rand(100, 32, 128, 1)\n",
    "train_labels = np.random.randint(36, size=(100, 32))\n",
    "val_images = np.random.rand(20, 32, 128, 1)\n",
    "val_labels = np.random.randint(36, size=(20, 32))\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=10, validation_data=(val_images, val_labels))\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3507a8a8-6835-4480-8b22-f936cdea1736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_license_plate(image):\n",
    "    # Detect license plate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        boxes, scores, labels = model([image.to(device)])\n",
    "    box = boxes[0][0].cpu().numpy().astype(int)\n",
    "    xmin, ymin, xmax, ymax = box\n",
    "\n",
    "    # Crop detected license plate\n",
    "    plate_image = image[ymin:ymax, xmin:xmax]\n",
    "    plate_image_resized = cv2.resize(plate_image, (128, 32))\n",
    "\n",
    "    # Recognize text from cropped license plate\n",
    "    text = model_ocr.predict(np.expand_dims(plate_image_resized, axis=0))\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "test_image = images['test.jpg']\n",
    "recognized_text = recognize_license_plate(test_image)\n",
    "print(recognized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed91a1-d568-4381-93bd-35861f49d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "# Calculate IoU for a test image\n",
    "detected_box = [xmin, ymin, xmax, ymax]  # From the detection model\n",
    "true_box = annotations[annotations['filename'] == 'test.jpg'][['xmin', 'ymin', 'xmax', 'ymax']].values[0]\n",
    "\n",
    "print(\"IoU:\", iou(detected_box, true_box))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c44a163-5206-457a-b70e-2d300272e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_accuracy(predicted_text, true_text):\n",
    "    correct_chars = sum(p == t for p, t in zip(predicted_text, true_text))\n",
    "    return correct_chars / len(true_text)\n",
    "\n",
    "def word_accuracy(predicted_text, true_text):\n",
    "    return int(predicted_text == true_text)\n",
    "\n",
    "# Calculate accuracy for a test image\n",
    "recognized_text = \"ABC123\"  # From the recognition model\n",
    "true_text = \"ABC123\"  # From the ground truth\n",
    "\n",
    "print(\"Character Accuracy:\", character_accuracy(recognized_text, true_text))\n",
    "print(\"Word Accuracy:\", word_accuracy(recognized_text, true_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3095f697-c2ce-4352-9bc8-d4668a55b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_accuracy = []\n",
    "for filename in test_images:\n",
    "    test_image = images[filename]\n",
    "    true_text = annotations[annotations['filename'] == filename]['text'].values[0]\n",
    "    recognized_text = recognize_license_plate(test_image)\n",
    "    \n",
    "    total_accuracy.append(character_accuracy(recognized_text, true_text))\n",
    "\n",
    "print(\"Overall Character Accuracy:\", sum(total_accuracy) / len(total_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057686a6-0466-4c22-af92-02f371cf8edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9539fb9f-dee9-4b0a-8cb8-9a1183bba081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029534af-d2db-439b-ae17-cd7c13b6615c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
